{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Explainability Tutorial\n",
    "\n",
    "## Implementing Chefer et al. for Vision-Language Model Interpretability\n",
    "\n",
    "This tutorial demonstrates how to generate explanations for MedGemma predictions using the gradient-weighted attention method from Chefer et al. (2021).\n",
    "\n",
    "**Reference:** Chefer, H., Gur, S., & Wolf, L. (2021). *Transformer Interpretability Beyond Attention Visualization.* CVPR 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction: Why Explainability Matters](#1-introduction)\n",
    "2. [Method Overview: The Chefer Approach](#2-method-overview)\n",
    "3. [Setup and Dependencies](#3-setup)\n",
    "4. [MedGemma Architecture Overview](#4-architecture)\n",
    "5. [Loading MedGemma](#5-loading-model)\n",
    "6. [Basic Usage with Test Images](#6-basic-usage)\n",
    "7. [Step-by-Step Explanation Pipeline](#7-step-by-step)\n",
    "8. [Medical Imaging: Chest X-ray Analysis](#8-chest-xray)\n",
    "9. [Comparing Methods](#9-comparing-methods)\n",
    "10. [Region Analysis](#10-region-analysis)\n",
    "11. [Advanced: Layer-wise Analysis](#11-layer-analysis)\n",
    "12. [Limitations and Considerations](#12-limitations)\n",
    "13. [Summary](#13-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Explainability Matters <a name=\"1-introduction\"></a>\n",
    "\n",
    "Medical AI systems require **transparency** and **interpretability** for several reasons:\n",
    "\n",
    "1. **Clinical Trust**: Physicians need to understand *why* a model made a prediction to trust and act on it\n",
    "2. **Regulatory Compliance**: Medical devices require explainability for FDA approval\n",
    "3. **Error Detection**: Understanding model attention helps identify when it's focusing on irrelevant features\n",
    "4. **Bias Detection**: Explanations can reveal if a model relies on spurious correlations\n",
    "\n",
    "This tutorial shows how to generate **relevancy maps** that highlight which parts of an image and text prompt contribute to MedGemma's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Method Overview: The Chefer Approach <a name=\"2-method-overview\"></a>\n\n### Key Insight\nRaw attention weights show where the model *looks*, but not whether that attention *matters* for the final prediction. The Chefer method combines attention with **gradients** to identify truly important regions.\n\n### Core Equations\n\n**Equation 5: Gradient-Weighted Attention**\n$$\\bar{A} = \\mathbb{E}_h \\left[ (\\nabla A \\odot A)^+ \\right]$$\n\nWhere:\n- $A$ = attention weights\n- $\\nabla A$ = gradient of the loss w.r.t. attention\n- $\\odot$ = element-wise multiplication\n- $(\\cdot)^+$ = keeping only positive values\n- $\\mathbb{E}_h$ = averaging over attention heads\n\n**Equation 6: Relevancy Propagation**\n$$R = \\bar{A} \\cdot R$$\n\nStarting with $R = I$ (identity matrix), we propagate relevancy through each layer.\n\n### Critical: Correct Backprop Target for Causal LM\n\n**This is the most important insight for correct explanations:**\n\nFor causal language models like MedGemma:\n- **Logit at position i predicts token at position i+1**\n- To explain why token at position p was generated:\n  1. Backprop from logit at position **p-1**\n  2. Use the **actual token id** at position p (not argmax)\n  3. Extract relevancy from row **p-1** in the R matrix\n\n**Wrong approach (common mistake):**\n```python\n# WRONG: Using last position with argmax\ntarget_logit = logits[0, -1, logits[0, -1].argmax()]\n```\n\n**Correct approach:**\n```python\n# CORRECT: For token at position p, use logit at p-1\nlogit_position = target_token_position - 1\ntarget_token_id = input_ids[0, target_token_position]  # Actual token\ntarget_logit = logits[0, logit_position, target_token_id]\n```\n\n### Why This Works\n- **Correct gradient flow**: Gradients reflect which attention patterns led to generating this specific token\n- **Actual token id**: Using the real token (not argmax) ensures we explain what actually happened\n- **Position p-1**: This is where the model \"decided\" to output the token at p"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup and Dependencies <a name=\"3-setup\"></a>\n",
    "\n",
    "First, let's set up HuggingFace authentication and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace authentication\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Get token from Colab secrets\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token, add_to_git_credential=False)\n",
    "print(\"HuggingFace authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MedGemma Architecture Overview <a name=\"4-architecture\"></a>\n",
    "\n",
    "MedGemma 1.5 4B is a vision-language model with:\n",
    "\n",
    "### Vision Encoder (SigLIP)\n",
    "- 27 transformer layers\n",
    "- Processes images to 4096 patches (64×64)\n",
    "- Pooled via 4×4 AvgPool2d to **256 image tokens** (16×16 grid)\n",
    "\n",
    "### Language Model (Gemma3)\n",
    "- **34 transformer layers**\n",
    "- Grouped-Query Attention (GQA): 8 query heads, 4 KV heads\n",
    "- 5:1 local:global attention ratio\n",
    "- Global layers at indices: 5, 11, 17, 23, 29\n",
    "- Local attention window: 1024 tokens\n",
    "\n",
    "### Token Structure\n",
    "```\n",
    "Position 0:     <bos>\n",
    "Position 1:     <start_of_turn>\n",
    "Position 2:     user\n",
    "Position 3:     \\n\\n\n",
    "Position 4-5:   <start_of_image> tokens\n",
    "Position 6-261: 256 IMAGE TOKENS (16×16 grid)\n",
    "Position 262:   <end_of_image>\n",
    "Position 263+:  Text prompt and generated response\n",
    "```\n",
    "\n",
    "**Important for Chest X-rays:**\n",
    "- Left side of image = Patient's RIGHT side\n",
    "- Right side of image = Patient's LEFT side\n",
    "- The 16×16 grid maps to anatomical regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading MedGemma <a name=\"5-loading-model\"></a>\n",
    "\n",
    "**Critical:** We must use `attn_implementation='eager'` to get attention outputs. SDPA (the default) doesn't support `output_attentions=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_name = \"google/medgemma-1.5-4b-it\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "print(\"(This may take 1-2 minutes)\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",  # REQUIRED for attention output!\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "\n",
    "# Print key config\n",
    "if hasattr(model.config, 'text_config'):\n",
    "    tc = model.config.text_config\n",
    "    print(f\"\\nLanguage Model Config:\")\n",
    "    print(f\"  Layers: {tc.num_hidden_layers}\")\n",
    "    print(f\"  Query heads: {tc.num_attention_heads}\")\n",
    "    print(f\"  KV heads: {tc.num_key_value_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Usage with Test Images <a name=\"6-basic-usage\"></a>\n",
    "\n",
    "Let's start with simple test images to understand how the explainer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the explainability library\n",
    "from medgemma_explainability import MedGemmaExplainer\n",
    "from medgemma_explainability.visualization import (\n",
    "    ExplanationResult,\n",
    "    visualize_explanation,\n",
    "    visualize_comparison,\n",
    ")\n",
    "from medgemma_explainability.relevancy import (\n",
    "    compute_Abar,\n",
    "    propagate_relevancy,\n",
    "    extract_token_relevancy,\n",
    "    split_relevancy,\n",
    "    compute_raw_attention_relevancy,\n",
    ")\n",
    "from medgemma_explainability.utils import (\n",
    "    normalize_relevancy,\n",
    "    reshape_image_relevancy,\n",
    "    is_global_layer,\n",
    ")\n",
    "\n",
    "# Create explainer\n",
    "explainer = MedGemmaExplainer(model, processor, device=device)\n",
    "print(\"Explainer created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test images\n",
    "def create_test_image(pattern='split'):\n",
    "    \"\"\"Create test images with different patterns.\"\"\"\n",
    "    img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    if pattern == 'split':\n",
    "        # Left red, right blue\n",
    "        img[:, :112, 0] = 255\n",
    "        img[:, 112:, 2] = 255\n",
    "    elif pattern == 'quadrant':\n",
    "        # Four colored quadrants\n",
    "        img[:112, :112, 0] = 255  # Red top-left\n",
    "        img[:112, 112:, 1] = 255  # Green top-right\n",
    "        img[112:, :112, 2] = 255  # Blue bottom-left\n",
    "        img[112:, 112:] = [255, 255, 0]  # Yellow bottom-right\n",
    "    elif pattern == 'circle':\n",
    "        # Red circle on blue background\n",
    "        y, x = np.ogrid[:224, :224]\n",
    "        mask = (x - 112)**2 + (y - 112)**2 <= 50**2\n",
    "        img[mask] = [255, 0, 0]\n",
    "        img[~mask] = [0, 0, 255]\n",
    "    \n",
    "    return Image.fromarray(img)\n",
    "\n",
    "# Display test images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for ax, pattern in zip(axes, ['split', 'quadrant', 'circle']):\n",
    "    ax.imshow(create_test_image(pattern))\n",
    "    ax.set_title(f'{pattern.capitalize()} Pattern')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick test with the high-level API\ntest_image = create_test_image('split')\nprompt = \"What colors do you see in this image?\"\n\nprint(\"Running explanation...\")\n\n# The explain() method now uses the CORRECT backprop target:\n# - For causal LM, logit at position i predicts token at i+1\n# - To explain token at position p, backprop from logit at p-1\nresult = explainer.explain(test_image, prompt, max_new_tokens=50)\n\nprint(f\"\\nGenerated text: {result.generated_text}\")\nprint(f\"\\nImage relevancy shape: {result.image_relevancy.shape}\")\nprint(f\"Text relevancy shape: {result.text_relevancy.shape}\")\nprint(f\"\\nMetadata: {result.metadata}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig = visualize_explanation(test_image, result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# NEW FEATURE: Explain a specific keyword in the response\n# This is useful for understanding why the model generated specific words\n\nprint(\"Explaining specific keyword 'red'...\")\ntry:\n    result_keyword = explainer.explain_keyword(test_image, prompt, keyword=\"red\", max_new_tokens=50)\n    \n    print(f\"\\nGenerated text: {result_keyword.generated_text}\")\n    print(f\"Target token: {result_keyword.metadata.get('target_token', 'N/A')}\")\n    print(f\"Token position: {result_keyword.metadata.get('target_token_position', 'N/A')}\")\n    \n    # Visualize\n    fig = visualize_explanation(test_image, result_keyword, title_suffix=\" (keyword: 'red')\")\n    plt.show()\nexcept ValueError as e:\n    print(f\"Keyword not found: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step-by-Step Explanation Pipeline <a name=\"7-step-by-step\"></a>\n",
    "\n",
    "Let's break down the explanation process to understand each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for MedGemma token structure\n",
    "NUM_IMAGE_TOKENS = 256\n",
    "IMAGE_START_IDX = 6\n",
    "\n",
    "# Create test image and prompt\n",
    "test_image = create_test_image('quadrant')\n",
    "prompt = \"Describe this image in detail.\"\n",
    "\n",
    "# STEP 1: Prepare inputs\n",
    "print(\"STEP 1: Prepare inputs\")\n",
    "messages = [{'role': 'user', 'content': [\n",
    "    {'type': 'image', 'image': test_image},\n",
    "    {'type': 'text', 'text': prompt}\n",
    "]}]\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=text, images=test_image, return_tensors='pt').to(device)\n",
    "print(f\"  Input sequence length: {inputs['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Generate response\n",
    "print(\"STEP 2: Generate response\")\n",
    "with torch.no_grad():\n",
    "    gen_outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "generated_text = processor.decode(gen_outputs[0], skip_special_tokens=True)\n",
    "if 'model' in generated_text:\n",
    "    response = generated_text.split('model')[-1].strip()\n",
    "else:\n",
    "    response = generated_text\n",
    "print(f\"  Response: {response[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Forward pass with attention\n",
    "print(\"STEP 3: Forward pass with attention output\")\n",
    "model.train()  # Enable gradients\n",
    "\n",
    "full_inputs = {\n",
    "    'input_ids': gen_outputs,\n",
    "    'attention_mask': torch.ones_like(gen_outputs),\n",
    "    'pixel_values': inputs['pixel_values'],\n",
    "}\n",
    "\n",
    "outputs = model(**full_inputs, output_attentions=True, return_dict=True)\n",
    "\n",
    "# Retain gradients on attention tensors\n",
    "for attn in outputs.attentions:\n",
    "    if attn is not None and attn.requires_grad:\n",
    "        attn.retain_grad()\n",
    "\n",
    "print(f\"  Got {len(outputs.attentions)} attention layers\")\n",
    "print(f\"  Attention shape per layer: {outputs.attentions[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STEP 4: Compute attention gradients via backprop (CORRECT METHOD)\nprint(\"STEP 4: Compute attention gradients via backprop\")\nprint(\"  IMPORTANT: Using CORRECT backprop target for causal LM\")\n\n# Keep model in eval mode, use torch.enable_grad() context\nmodel.eval()\n\nwith torch.enable_grad():\n    # Re-run forward pass with gradients\n    outputs = model(**full_inputs, output_attentions=True, return_dict=True)\n    \n    # Retain gradients on attention tensors\n    for attn in outputs.attentions:\n        if attn is not None:\n            attn.requires_grad_(True)\n            attn.retain_grad()\n    \n    # CORRECT TARGET: To explain the last token, use logit at position -2\n    # Because logit[i] predicts token[i+1]\n    target_token_position = gen_outputs.shape[1] - 1  # Last token position\n    target_token_id = gen_outputs[0, target_token_position].item()  # Actual token id\n    logit_position = target_token_position - 1  # Logit that predicted this token\n    \n    target_logit = outputs.logits[0, logit_position, target_token_id]\n    print(f\"  Target token position: {target_token_position}\")\n    print(f\"  Logit position (p-1): {logit_position}\")\n    print(f\"  Target token: '{processor.decode([target_token_id])}'\")\n    \n    target_logit.backward(retain_graph=True)\n\n# Collect attention and gradients\nattention_maps = {}\nattention_grads = {}\n\nfor i, attn in enumerate(outputs.attentions):\n    if attn is not None:\n        attention_maps[i] = attn.detach().float()\n        if attn.grad is not None:\n            attention_grads[i] = attn.grad.detach().float()\n\nprint(f\"  Collected {len(attention_maps)} attention maps\")\nprint(f\"  Collected {len(attention_grads)} gradient maps\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Propagate relevancy (Chefer method)\n",
    "print(\"STEP 5: Propagate relevancy through layers\")\n",
    "\n",
    "seq_len = gen_outputs.shape[1]\n",
    "\n",
    "R = propagate_relevancy(\n",
    "    attention_maps,\n",
    "    attention_grads,\n",
    "    seq_len,\n",
    "    handle_local_attention=True,\n",
    "    local_window_size=1024,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"  Relevancy matrix shape: {R.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STEP 6: Extract token relevancy (from CORRECT position)\nprint(\"STEP 6: Extract token relevancy\")\nprint(f\"  IMPORTANT: Extract from row {logit_position} (the predicting position)\")\n\n# Extract relevancy from the PREDICTING position (p-1), not the target position\ntoken_relevancy = extract_token_relevancy(R, target_token_idx=logit_position)\nimage_relevancy, text_relevancy = split_relevancy(\n    token_relevancy, NUM_IMAGE_TOKENS, IMAGE_START_IDX\n)\n\n# Reshape to 16x16 grid and normalize\nimage_relevancy_2d = reshape_image_relevancy(image_relevancy)\nimage_relevancy_2d = normalize_relevancy(image_relevancy_2d)\ntext_relevancy_norm = normalize_relevancy(text_relevancy)\n\nprint(f\"  Image relevancy shape: {image_relevancy_2d.shape}\")\nprint(f\"  Text relevancy tokens: {len(text_relevancy)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Visualize results\n",
    "print(\"STEP 7: Visualize\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(test_image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Overlay heatmap\n",
    "heatmap = image_relevancy_2d.cpu().numpy()\n",
    "heatmap_resized = np.array(Image.fromarray(heatmap).resize(test_image.size, Image.BILINEAR))\n",
    "axes[1].imshow(test_image)\n",
    "im = axes[1].imshow(heatmap_resized, cmap='jet', alpha=0.5)\n",
    "axes[1].set_title('Relevancy Heatmap Overlay')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Text token relevancy\n",
    "text_start_idx = IMAGE_START_IDX + NUM_IMAGE_TOKENS\n",
    "token_labels = [processor.decode([t.item()]) for t in gen_outputs[0][text_start_idx:]]\n",
    "text_rel_np = text_relevancy_norm.cpu().numpy()\n",
    "\n",
    "n_tokens = min(15, len(text_rel_np))\n",
    "sorted_idx = np.argsort(text_rel_np)[::-1][:n_tokens]\n",
    "labels = [token_labels[i][:15].replace('\\n', '\\\\n') for i in sorted_idx]\n",
    "values = [text_rel_np[i] for i in sorted_idx]\n",
    "\n",
    "axes[2].barh(range(len(labels)), values, color='steelblue')\n",
    "axes[2].set_yticks(range(len(labels)))\n",
    "axes[2].set_yticklabels(labels, fontsize=9)\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].set_xlabel('Relevancy')\n",
    "axes[2].set_title('Top Text Token Relevancy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Medical Imaging: Chest X-ray Analysis <a name=\"8-chest-xray\"></a>\n",
    "\n",
    "Now let's apply the explainability method to a real medical image - a chest X-ray with pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chest X-ray image\n",
    "# Option 1: Use local file if available\n",
    "import os\n",
    "\n",
    "if os.path.exists('/content/chest_xray.jpg'):\n",
    "    chest_xray = Image.open('/content/chest_xray.jpg').convert('RGB')\n",
    "    print(\"Loaded local chest X-ray\")\n",
    "else:\n",
    "    # Option 2: Download from URL\n",
    "    url = \"https://prod-images-static.radiopaedia.org/images/1371188/0a1f5edc85aa58d5780928cb39b08659c1fc4d6d7c7dce2f8db1d63c7c737234_big_gallery.jpeg\"\n",
    "    response = requests.get(url)\n",
    "    chest_xray = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    print(\"Downloaded chest X-ray from URL\")\n",
    "\n",
    "print(f\"Image size: {chest_xray.size}\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(chest_xray, cmap='gray')\n",
    "plt.title('Chest X-ray (PA View)\\nLeft of image = Patient RIGHT side')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chest X-ray for pneumonia\n",
    "medical_prompt = \"Analyze this chest X-ray. Is there evidence of pneumonia? If so, describe the location and appearance of the consolidation.\"\n",
    "\n",
    "print(\"Analyzing chest X-ray...\")\n",
    "print(f\"Prompt: {medical_prompt}\")\n",
    "print()\n",
    "\n",
    "# Prepare inputs\n",
    "messages = [{'role': 'user', 'content': [\n",
    "    {'type': 'image', 'image': chest_xray},\n",
    "    {'type': 'text', 'text': medical_prompt}\n",
    "]}]\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=text, images=chest_xray, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    gen_outputs = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "\n",
    "generated_text = processor.decode(gen_outputs[0], skip_special_tokens=True)\n",
    "if 'model' in generated_text:\n",
    "    response = generated_text.split('model')[-1].strip()\n",
    "else:\n",
    "    response = generated_text\n",
    "\n",
    "print(\"MedGemma's Assessment:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention and gradients for chest X-ray\n",
    "print(\"Extracting attention and gradients...\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "full_inputs = {\n",
    "    'input_ids': gen_outputs,\n",
    "    'attention_mask': torch.ones_like(gen_outputs),\n",
    "    'pixel_values': inputs['pixel_values'],\n",
    "}\n",
    "\n",
    "outputs = model(**full_inputs, output_attentions=True, return_dict=True)\n",
    "\n",
    "for attn in outputs.attentions:\n",
    "    if attn is not None and attn.requires_grad:\n",
    "        attn.retain_grad()\n",
    "\n",
    "# Backward pass\n",
    "logits = outputs.logits\n",
    "target_logit = logits[0, -1, logits[0, -1].argmax()]\n",
    "target_logit.backward(retain_graph=True)\n",
    "\n",
    "# Collect\n",
    "attention_maps = {}\n",
    "attention_grads = {}\n",
    "\n",
    "for i, attn in enumerate(outputs.attentions):\n",
    "    if attn is not None:\n",
    "        attention_maps[i] = attn.detach()\n",
    "        if attn.grad is not None:\n",
    "            attention_grads[i] = attn.grad.detach()\n",
    "\n",
    "print(f\"Collected {len(attention_grads)} gradient maps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate relevancy\n",
    "seq_len = gen_outputs.shape[1]\n",
    "\n",
    "R = propagate_relevancy(\n",
    "    attention_maps,\n",
    "    attention_grads,\n",
    "    seq_len,\n",
    "    handle_local_attention=True,\n",
    "    local_window_size=1024,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Extract relevancy\n",
    "token_relevancy = extract_token_relevancy(R, target_token_idx=-1)\n",
    "image_relevancy, text_relevancy = split_relevancy(\n",
    "    token_relevancy, NUM_IMAGE_TOKENS, IMAGE_START_IDX\n",
    ")\n",
    "\n",
    "image_relevancy_2d = reshape_image_relevancy(image_relevancy)\n",
    "image_relevancy_2d = normalize_relevancy(image_relevancy_2d)\n",
    "text_relevancy = normalize_relevancy(text_relevancy)\n",
    "\n",
    "# Also compute raw attention baseline\n",
    "raw_attn = compute_raw_attention_relevancy(attention_maps)\n",
    "raw_token_rel = raw_attn[-1, :]\n",
    "raw_img, raw_txt = split_relevancy(raw_token_rel, NUM_IMAGE_TOKENS, IMAGE_START_IDX)\n",
    "raw_image_2d = normalize_relevancy(reshape_image_relevancy(raw_img))\n",
    "\n",
    "print(\"Relevancy computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive chest X-ray visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Images\n",
    "axes[0, 0].imshow(chest_xray, cmap='gray')\n",
    "axes[0, 0].set_title('Original Chest X-ray', fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Chefer method heatmap\n",
    "heatmap = image_relevancy_2d.cpu().numpy()\n",
    "heatmap_resized = np.array(Image.fromarray(heatmap).resize(chest_xray.size, Image.BILINEAR))\n",
    "\n",
    "axes[0, 1].imshow(chest_xray, cmap='gray')\n",
    "im1 = axes[0, 1].imshow(heatmap_resized, cmap='jet', alpha=0.5)\n",
    "axes[0, 1].set_title('Chefer Method - Relevancy Map', fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "# Raw attention\n",
    "raw_heatmap = raw_image_2d.cpu().numpy()\n",
    "raw_resized = np.array(Image.fromarray(raw_heatmap).resize(chest_xray.size, Image.BILINEAR))\n",
    "\n",
    "axes[0, 2].imshow(chest_xray, cmap='gray')\n",
    "im2 = axes[0, 2].imshow(raw_resized, cmap='jet', alpha=0.5)\n",
    "axes[0, 2].set_title('Raw Attention (Last Layer)', fontsize=14)\n",
    "axes[0, 2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "# Row 2: Analysis\n",
    "# Heatmap grid\n",
    "axes[1, 0].imshow(heatmap, cmap='hot')\n",
    "axes[1, 0].set_title('Relevancy Heatmap (16x16 grid)', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Image column')\n",
    "axes[1, 0].set_ylabel('Image row')\n",
    "plt.colorbar(axes[1, 0].images[0], ax=axes[1, 0], fraction=0.046)\n",
    "\n",
    "# Region analysis (with correct anatomical orientation!)\n",
    "# Left of image = Patient's RIGHT side\n",
    "regions = {\n",
    "    'Pt Right Upper': heatmap[:5, :8].mean(),\n",
    "    'Pt Right Middle': heatmap[5:11, :8].mean(),\n",
    "    'Pt Right Lower': heatmap[11:, :8].mean(),\n",
    "    'Pt Left Upper': heatmap[:5, 8:].mean(),\n",
    "    'Pt Left Middle': heatmap[5:11, 8:].mean(),\n",
    "    'Pt Left Lower': heatmap[11:, 8:].mean(),\n",
    "}\n",
    "\n",
    "colors = ['red' if 'Pt Right' in k else 'steelblue' for k in regions.keys()]\n",
    "bars = axes[1, 1].barh(list(regions.keys()), list(regions.values()), color=colors)\n",
    "axes[1, 1].set_xlabel('Mean Relevancy')\n",
    "axes[1, 1].set_title('Relevancy by Lung Region\\n(Red = Pneumonia side)', fontsize=14)\n",
    "\n",
    "for bar, (name, val) in zip(bars, regions.items()):\n",
    "    axes[1, 1].text(val + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Top text tokens\n",
    "text_start_idx = IMAGE_START_IDX + NUM_IMAGE_TOKENS\n",
    "token_labels = [processor.decode([t.item()]) for t in gen_outputs[0][text_start_idx:]]\n",
    "text_rel_np = text_relevancy.cpu().numpy()\n",
    "\n",
    "n_tokens = min(15, len(text_rel_np))\n",
    "sorted_idx = np.argsort(text_rel_np)[::-1][:n_tokens]\n",
    "labels = [token_labels[i][:15].replace('\\n', '\\\\n') for i in sorted_idx]\n",
    "values = [text_rel_np[i] for i in sorted_idx]\n",
    "\n",
    "axes[1, 2].barh(range(len(labels)), values, color='steelblue')\n",
    "axes[1, 2].set_yticks(range(len(labels)))\n",
    "axes[1, 2].set_yticklabels(labels, fontsize=9)\n",
    "axes[1, 2].invert_yaxis()\n",
    "axes[1, 2].set_xlabel('Relevancy Score')\n",
    "axes[1, 2].set_title('Top Text Token Relevancy', fontsize=14)\n",
    "\n",
    "plt.suptitle('MedGemma Explainability: Chest X-ray Pneumonia Analysis', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print region analysis\n",
    "print(\"\\nRegion Analysis (Left of image = Patient's RIGHT side):\")\n",
    "print(\"-\" * 50)\n",
    "for region, value in sorted(regions.items(), key=lambda x: -x[1]):\n",
    "    marker = \" <-- PNEUMONIA SIDE\" if 'Pt Right' in region else \"\"\n",
    "    print(f\"  {region}: {value:.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparing Methods <a name=\"9-comparing-methods\"></a>\n",
    "\n",
    "Let's compare different explainability approaches:\n",
    "1. **Standard Chefer** (all layers with gradients)\n",
    "2. **Global Layers Only** (layers 5, 11, 17, 23, 29)\n",
    "3. **Attention Rollout** (no gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(attention_maps, add_residual=True):\n",
    "    \"\"\"Compute attention rollout (Abnar & Zuidema, 2020).\"\"\"\n",
    "    layer_indices = sorted(attention_maps.keys())\n",
    "    seq_len = attention_maps[layer_indices[0]].shape[-1]\n",
    "    device = attention_maps[layer_indices[0]].device\n",
    "    \n",
    "    rollout = torch.eye(seq_len, device=device, dtype=torch.float32)\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        attn = attention_maps[layer_idx].float().mean(dim=(0, 1))\n",
    "        \n",
    "        if add_residual:\n",
    "            attn = 0.5 * attn + 0.5 * torch.eye(seq_len, device=device)\n",
    "            attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        rollout = torch.matmul(attn, rollout)\n",
    "    \n",
    "    return rollout\n",
    "\n",
    "\n",
    "def global_layers_only_relevancy(attention_maps, attention_grads, seq_len, device):\n",
    "    \"\"\"Compute relevancy using only global attention layers.\"\"\"\n",
    "    R = torch.eye(seq_len, device=device, dtype=torch.float32)\n",
    "    \n",
    "    global_layers = [i for i in sorted(attention_maps.keys()) if is_global_layer(i)]\n",
    "    \n",
    "    for layer_idx in global_layers:\n",
    "        if layer_idx not in attention_grads:\n",
    "            continue\n",
    "        \n",
    "        A = attention_maps[layer_idx].float()\n",
    "        grad_A = attention_grads[layer_idx].float()\n",
    "        \n",
    "        Abar = compute_Abar(A, grad_A, normalize=True)\n",
    "        Abar = Abar + torch.eye(seq_len, device=device, dtype=torch.float32)\n",
    "        Abar = Abar / Abar.sum(dim=-1, keepdim=True).clamp(min=1e-8)\n",
    "        \n",
    "        R = torch.matmul(Abar, R)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all three methods\n",
    "print(\"Computing comparison methods...\")\n",
    "\n",
    "# Method 1: Standard Chefer (already computed)\n",
    "img_2d_std = image_relevancy_2d\n",
    "\n",
    "# Method 2: Global layers only\n",
    "R_global = global_layers_only_relevancy(attention_maps, attention_grads, seq_len, device)\n",
    "token_rel_global = extract_token_relevancy(R_global, -1)\n",
    "img_rel_global, _ = split_relevancy(token_rel_global, NUM_IMAGE_TOKENS, IMAGE_START_IDX)\n",
    "img_2d_global = normalize_relevancy(reshape_image_relevancy(img_rel_global))\n",
    "\n",
    "# Method 3: Attention rollout\n",
    "R_rollout = attention_rollout(attention_maps, add_residual=True)\n",
    "token_rel_rollout = extract_token_relevancy(R_rollout, -1)\n",
    "img_rel_rollout, _ = split_relevancy(token_rel_rollout, NUM_IMAGE_TOKENS, IMAGE_START_IDX)\n",
    "img_2d_rollout = normalize_relevancy(reshape_image_relevancy(img_rel_rollout))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "def overlay_heatmap(ax, img, heatmap_tensor, title):\n",
    "    hm = heatmap_tensor.cpu().numpy() if torch.is_tensor(heatmap_tensor) else heatmap_tensor\n",
    "    hm_resized = np.array(Image.fromarray(hm.astype(np.float32)).resize(img.size, Image.BILINEAR))\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    im = ax.imshow(hm_resized, cmap='jet', alpha=0.5)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.axis('off')\n",
    "    return im\n",
    "\n",
    "axes[0, 0].imshow(chest_xray, cmap='gray')\n",
    "axes[0, 0].set_title('Original Chest X-ray', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "overlay_heatmap(axes[0, 1], chest_xray, img_2d_std, \n",
    "                'Method 1: Standard Chefer\\n(All 34 layers + gradients)')\n",
    "\n",
    "overlay_heatmap(axes[1, 0], chest_xray, img_2d_global,\n",
    "                'Method 2: Global Layers Only\\n(Layers 5, 11, 17, 23, 29)')\n",
    "\n",
    "overlay_heatmap(axes[1, 1], chest_xray, img_2d_rollout,\n",
    "                'Method 3: Attention Rollout\\n(No gradients)')\n",
    "\n",
    "plt.suptitle('Comparison of Explainability Methods', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Region Analysis <a name=\"10-region-analysis\"></a>\n",
    "\n",
    "Let's quantify the relevancy distribution across anatomical regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_regions(heatmap_tensor, name):\n",
    "    \"\"\"Analyze relevancy by lung region.\"\"\"\n",
    "    hm = heatmap_tensor.cpu().numpy() if torch.is_tensor(heatmap_tensor) else heatmap_tensor\n",
    "    \n",
    "    # Remember: Left of image = Patient's RIGHT side\n",
    "    regions = {\n",
    "        'Pt Right Upper': hm[:5, :8].mean(),\n",
    "        'Pt Right Middle': hm[5:11, :8].mean(),\n",
    "        'Pt Right Lower': hm[11:, :8].mean(),\n",
    "        'Pt Left Upper': hm[:5, 8:].mean(),\n",
    "        'Pt Left Middle': hm[5:11, 8:].mean(),\n",
    "        'Pt Left Lower': hm[11:, 8:].mean(),\n",
    "    }\n",
    "    \n",
    "    right_avg = (regions['Pt Right Upper'] + regions['Pt Right Middle'] + regions['Pt Right Lower']) / 3\n",
    "    left_avg = (regions['Pt Left Upper'] + regions['Pt Left Middle'] + regions['Pt Left Lower']) / 3\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'regions': regions,\n",
    "        'right_avg': right_avg,\n",
    "        'left_avg': left_avg,\n",
    "        'ratio': right_avg / left_avg if left_avg > 0 else float('inf'),\n",
    "    }\n",
    "\n",
    "# Analyze all methods\n",
    "methods = [\n",
    "    ('Standard Chefer', img_2d_std),\n",
    "    ('Global Only', img_2d_global),\n",
    "    ('Rollout', img_2d_rollout),\n",
    "    ('Raw Attention', raw_image_2d),\n",
    "]\n",
    "\n",
    "results = [analyze_regions(hm, name) for name, hm in methods]\n",
    "\n",
    "# Print results\n",
    "print(\"Region Analysis Results\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: Left of image = Patient's RIGHT side (pneumonia location)\\n\")\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Patient's Right side (avg): {r['right_avg']:.4f}\")\n",
    "    print(f\"  Patient's Left side (avg):  {r['left_avg']:.4f}\")\n",
    "    print(f\"  Right/Left Ratio: {r['ratio']:.2f}x\")\n",
    "    print(f\"  Best region: {max(r['regions'], key=r['regions'].get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize region comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(6)\n",
    "width = 0.2\n",
    "region_names = list(results[0]['regions'].keys())\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    values = [r['regions'][name] for name in region_names]\n",
    "    ax.bar(x + i*width, values, width, label=r['name'])\n",
    "\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(region_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Mean Relevancy')\n",
    "ax.set_title('Region Relevancy Comparison Across Methods')\n",
    "ax.legend()\n",
    "ax.axvline(x=2.5, color='gray', linestyle='--', alpha=0.5, label='Patient midline')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Layer-wise Analysis <a name=\"11-layer-analysis\"></a>\n",
    "\n",
    "Let's examine how attention to image tokens varies across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention to image tokens per layer\n",
    "layer_stats = []\n",
    "\n",
    "for layer_idx in sorted(attention_maps.keys()):\n",
    "    attn = attention_maps[layer_idx].float().mean(dim=(0, 1))  # Average over heads\n",
    "    \n",
    "    # Attention from last token to image tokens\n",
    "    last_token_attn = attn[-1, IMAGE_START_IDX:IMAGE_START_IDX + NUM_IMAGE_TOKENS]\n",
    "    img_attn_sum = last_token_attn.sum().item()\n",
    "    \n",
    "    layer_stats.append({\n",
    "        'layer': layer_idx,\n",
    "        'img_attention': img_attn_sum,\n",
    "        'is_global': is_global_layer(layer_idx),\n",
    "    })\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "layers = [s['layer'] for s in layer_stats]\n",
    "img_attn = [s['img_attention'] for s in layer_stats]\n",
    "colors = ['red' if s['is_global'] else 'steelblue' for s in layer_stats]\n",
    "\n",
    "bars = ax.bar(layers, img_attn, color=colors)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Attention to Image Tokens')\n",
    "ax.set_title('Attention to Image Tokens by Layer\\n(Red = Global layers, Blue = Local layers)')\n",
    "\n",
    "# Highlight top layers\n",
    "top_layers = sorted(layer_stats, key=lambda x: -x['img_attention'])[:5]\n",
    "print(\"Top 5 layers by image attention:\")\n",
    "for s in top_layers:\n",
    "    global_str = \" (GLOBAL)\" if s['is_global'] else \"\"\n",
    "    print(f\"  Layer {s['layer']}: {s['img_attention']*100:.1f}%{global_str}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Summary <a name=\"13-summary\"></a>\n\nIn this tutorial, we:\n\n1. **Loaded MedGemma** with eager attention for interpretability\n2. **Understood the architecture** (34 layers, 256 image tokens, local/global attention)\n3. **Implemented Chefer et al. method** with **correct backprop targets**\n4. **Applied to medical imaging** (chest X-ray pneumonia detection)\n5. **Compared multiple methods** (Chefer, global-only, rollout, raw attention)\n6. **Analyzed anatomical regions** with correct orientation\n7. **Examined layer-wise attention** patterns\n\n### Critical Insight: Correct Backprop Target\n\nThe most important fix for accurate explanations:\n\n```python\n# For causal LM: logit at position i predicts token at i+1\n# To explain token at position p:\nlogit_position = p - 1\ntarget_token_id = input_ids[0, p]  # Actual token, NOT argmax\ntarget_logit = logits[0, logit_position, target_token_id]\n# Extract relevancy from R[logit_position, :]\n```\n\n### New API Methods\n\nThe updated `MedGemmaExplainer` provides:\n- `explain(image, prompt, target_token_position)` - Explain a specific token\n- `explain_keyword(image, prompt, keyword)` - Explain a keyword in the response  \n- `explain_answer_span(image, prompt)` - Explain the entire answer\n\n### Key Findings\n- The Chefer method correctly highlights the patient's right lung field for pneumonia\n- Global attention layers (5, 11, 17, 23, 29) have strongest image attention\n- Multiple methods should be compared for robust interpretability\n\n### Files Included\n- `medgemma_explainability/` - Core library (v0.3.0)\n- `scripts/` - Standalone analysis scripts\n- `tests/` - Unit tests\n- `outputs/` - Generated visualizations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary <a name=\"13-summary\"></a>\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. **Loaded MedGemma** with eager attention for interpretability\n",
    "2. **Understood the architecture** (34 layers, 256 image tokens, local/global attention)\n",
    "3. **Implemented Chefer et al. method** for gradient-weighted attention\n",
    "4. **Applied to medical imaging** (chest X-ray pneumonia detection)\n",
    "5. **Compared multiple methods** (Chefer, global-only, rollout, raw attention)\n",
    "6. **Analyzed anatomical regions** with correct orientation\n",
    "7. **Examined layer-wise attention** patterns\n",
    "\n",
    "### Key Findings\n",
    "- The Chefer method correctly highlights the patient's right lung field for pneumonia\n",
    "- Global attention layers (5, 11, 17, 23, 29) have strongest image attention\n",
    "- Multiple methods should be compared for robust interpretability\n",
    "\n",
    "### Files Included\n",
    "- `medgemma_explainability/` - Core library\n",
    "- `scripts/` - Standalone analysis scripts\n",
    "- `tests/` - Unit tests\n",
    "- `outputs/` - Generated visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Tutorial complete!\")\n",
    "print(\"\\nGenerated outputs are saved in the /content/outputs/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}