{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Explainability - Setup and Verification\n",
    "\n",
    "This notebook verifies that MedGemma can be loaded and attention weights can be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup HuggingFace Authentication\n",
    "\n",
    "Make sure you have:\n",
    "1. Created a HuggingFace account\n",
    "2. Accepted the MedGemma license at https://huggingface.co/google/medgemma-1.5-4b-it\n",
    "3. Created an access token at https://huggingface.co/settings/tokens\n",
    "4. Added the token to Colab secrets with name `HF_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace authentication\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Get token from Colab secrets\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token\n",
    "\n",
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token, add_to_git_credential=False)\n",
    "print(\"HuggingFace authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load MedGemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_name = \"google/medgemma-1.5-4b-it\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"Processor loaded\")\n",
    "\n",
    "# Load model in bfloat16\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_structure(model, max_depth=3):\n",
    "    \"\"\"Print model structure.\"\"\"\n",
    "    def _print(module, prefix=\"\", depth=0):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        for i, (name, child) in enumerate(module.named_children()):\n",
    "            is_last = i == len(list(module.named_children())) - 1\n",
    "            print(f\"{prefix}{'└── ' if is_last else '├── '}{name}: {child.__class__.__name__}\")\n",
    "            _print(child, prefix + ('    ' if is_last else '│   '), depth + 1)\n",
    "    \n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    _print(model)\n",
    "\n",
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model config\n",
    "config = model.config\n",
    "print(\"Model Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(config, 'text_config'):\n",
    "    tc = config.text_config\n",
    "    print(\"\\nLanguage Model:\")\n",
    "    print(f\"  num_hidden_layers: {getattr(tc, 'num_hidden_layers', 'N/A')}\")\n",
    "    print(f\"  num_attention_heads: {getattr(tc, 'num_attention_heads', 'N/A')}\")\n",
    "    print(f\"  num_key_value_heads: {getattr(tc, 'num_key_value_heads', 'N/A')}\")\n",
    "    print(f\"  hidden_size: {getattr(tc, 'hidden_size', 'N/A')}\")\n",
    "    print(f\"  head_dim: {getattr(tc, 'head_dim', 'N/A')}\")\n",
    "\n",
    "if hasattr(config, 'vision_config'):\n",
    "    vc = config.vision_config\n",
    "    print(\"\\nVision Model:\")\n",
    "    print(f\"  num_hidden_layers: {getattr(vc, 'num_hidden_layers', 'N/A')}\")\n",
    "    print(f\"  num_attention_heads: {getattr(vc, 'num_attention_heads', 'N/A')}\")\n",
    "    print(f\"  hidden_size: {getattr(vc, 'hidden_size', 'N/A')}\")\n",
    "    print(f\"  image_size: {getattr(vc, 'image_size', 'N/A')}\")\n",
    "    print(f\"  patch_size: {getattr(vc, 'patch_size', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Find Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all attention modules\n",
    "language_attn = []\n",
    "vision_attn = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if 'self_attn' in name.lower() or 'attention' in module.__class__.__name__.lower():\n",
    "        if 'language' in name.lower():\n",
    "            language_attn.append((name, module))\n",
    "        elif 'vision' in name.lower():\n",
    "            vision_attn.append((name, module))\n",
    "\n",
    "print(f\"Language model attention layers: {len(language_attn)}\")\n",
    "print(f\"Vision model attention layers: {len(vision_attn)}\")\n",
    "\n",
    "if language_attn:\n",
    "    print(f\"\\nFirst language attention: {language_attn[0][0]}\")\n",
    "    module = language_attn[0][1]\n",
    "    print(f\"  Class: {module.__class__.__name__}\")\n",
    "    for attr in ['num_heads', 'num_key_value_heads', 'head_dim']:\n",
    "        if hasattr(module, attr):\n",
    "            print(f\"  {attr}: {getattr(module, attr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create test image (left=red, right=blue)\n",
    "img_array = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "img_array[:, :112, 0] = 255  # Red on left\n",
    "img_array[:, 112:, 2] = 255  # Blue on right\n",
    "test_image = Image.fromarray(img_array)\n",
    "\n",
    "# Display test image\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "prompt = \"What colors do you see in this image?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": test_image},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=text, images=test_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "for k, v in inputs.items():\n",
    "    if hasattr(v, 'shape'):\n",
    "        print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "generated = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nGenerated response:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with output_attentions=True\n",
    "print(\"Testing attention extraction with output_attentions=True...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True, return_dict=True)\n",
    "\n",
    "if hasattr(outputs, 'attentions') and outputs.attentions is not None:\n",
    "    attentions = outputs.attentions\n",
    "    print(f\"\\nSUCCESS! Got {len(attentions)} attention tensors\")\n",
    "    print(f\"\\nFirst attention shape: {attentions[0].shape}\")\n",
    "    print(f\"Last attention shape: {attentions[-1].shape}\")\n",
    "    \n",
    "    # Analyze first attention tensor\n",
    "    first_attn = attentions[0]\n",
    "    print(f\"\\nFirst attention stats:\")\n",
    "    print(f\"  Shape: {first_attn.shape}\")\n",
    "    print(f\"  dtype: {first_attn.dtype}\")\n",
    "    print(f\"  Min: {first_attn.min().item():.6f}\")\n",
    "    print(f\"  Max: {first_attn.max().item():.6f}\")\n",
    "    print(f\"  Mean: {first_attn.mean().item():.6f}\")\n",
    "    \n",
    "    # Check softmax (rows should sum to 1)\n",
    "    row_sums = first_attn[0, 0].sum(dim=-1)\n",
    "    print(f\"\\n  Row sums (should be ~1):\")\n",
    "    print(f\"    Min: {row_sums.min().item():.4f}\")\n",
    "    print(f\"    Max: {row_sums.max().item():.4f}\")\n",
    "else:\n",
    "    print(\"\\nAttentions not available in outputs\")\n",
    "    print(\"Will need to use hooks for attention extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check attention shape details\n",
    "if hasattr(outputs, 'attentions') and outputs.attentions:\n",
    "    print(\"Attention tensor shapes across layers:\")\n",
    "    print(\"(batch, heads, seq_len, seq_len)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, attn in enumerate(outputs.attentions):\n",
    "        if i < 5 or i >= len(outputs.attentions) - 2:\n",
    "            print(f\"Layer {i:2d}: {tuple(attn.shape)}\")\n",
    "        elif i == 5:\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Gradient Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient capture\n",
    "print(\"Testing gradient capture...\")\n",
    "\n",
    "model.train()  # Enable gradients\n",
    "\n",
    "# Forward pass with gradients\n",
    "outputs = model(**inputs, output_attentions=True, return_dict=True)\n",
    "\n",
    "if outputs.attentions:\n",
    "    # Try to retain gradients on attention tensors\n",
    "    for attn in outputs.attentions:\n",
    "        if attn.requires_grad:\n",
    "            attn.retain_grad()\n",
    "    \n",
    "    # Backward from last token logit\n",
    "    logits = outputs.logits\n",
    "    target_logit = logits[0, -1, logits[0, -1].argmax()]\n",
    "    target_logit.backward(retain_graph=True)\n",
    "    \n",
    "    # Check gradients\n",
    "    grad_count = 0\n",
    "    for i, attn in enumerate(outputs.attentions):\n",
    "        if attn.grad is not None:\n",
    "            grad_count += 1\n",
    "            if i < 3:\n",
    "                print(f\"Layer {i}: grad shape {attn.grad.shape}, \"\n",
    "                      f\"grad norm {attn.grad.norm().item():.6f}\")\n",
    "    \n",
    "    print(f\"\\nGradients captured for {grad_count}/{len(outputs.attentions)} layers\")\n",
    "else:\n",
    "    print(\"No attentions available for gradient capture\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Verification Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"\\nLanguage model layers: {len(language_attn)}\")\n",
    "print(f\"Vision model layers: {len(vision_attn)}\")\n",
    "\n",
    "if hasattr(config, 'text_config'):\n",
    "    tc = config.text_config\n",
    "    print(f\"\\nGQA configuration:\")\n",
    "    print(f\"  Query heads: {getattr(tc, 'num_attention_heads', 'N/A')}\")\n",
    "    print(f\"  KV heads: {getattr(tc, 'num_key_value_heads', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nAttention extraction: {'Working' if hasattr(outputs, 'attentions') and outputs.attentions else 'Needs hooks'}\")\n",
    "print(f\"\\nReady for Chefer method implementation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and processor for later use\n",
    "# They're already loaded in memory, so we just confirm they're available\n",
    "print(f\"Model loaded: {model is not None}\")\n",
    "print(f\"Processor loaded: {processor is not None}\")\n",
    "print(f\"\\nYou can now proceed to the explainability implementation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
